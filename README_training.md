# Tunnel Detection Training Pipeline

This pipeline trains a U-Net model to detect and segment tunnels in density grids from quantum gravity sensing.

## Setup

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Generate Training Data
```bash
python density_grid_generator.py
```

This creates the `training_data/` directory with:
- `density_grid_XXX.npy` - Input density grids (60×150)
- `tunnel_mask_XXX.npy` - Ground truth segmentation masks (60×150)
- `metadata_XXX.json` - Labels (has_tunnel, num_tunnels, tunnel info)
- `sample_XXX.png` - Visualizations

By default, generates 50 samples with 0-3 tunnels each.

## Training

### Basic Training
```bash
python train.py
```

This will:
- Split data into 80% train / 20% validation
- Train a U-Net model for 100 epochs
- Save checkpoints to `checkpoints/`
- Log metrics to TensorBoard in `runs/`
- Save best model as `checkpoints/best_model.pth`

### Monitor Training with TensorBoard
```bash
tensorboard --logdir=runs
```

Then open http://localhost:6006 in your browser to view:
- Training/validation loss curves
- Dice score and IoU metrics
- Learning rate schedule

### Custom Training Parameters
```python
from train import train

train(
    data_dir='training_data',
    num_epochs=100,
    batch_size=8,
    learning_rate=1e-3,
    save_dir='checkpoints',
    log_dir='runs'
)
```

## Evaluation

### Evaluate Trained Model
```bash
python inference.py
```

This will:
- Load the best model from `checkpoints/best_model.pth`
- Evaluate on all samples in `training_data/`
- Save results to `evaluation_results/`
- Print metrics: classification accuracy, mean Dice, mean IoU

### Use Model for Prediction
```python
from inference import load_model, predict
import numpy as np

# Load model
model = load_model('checkpoints/best_model.pth')

# Load density grid
density_grid = np.load('training_data/density_grid_000.npy')

# Predict
prob_map, binary_mask, has_tunnel = predict(model, density_grid)

print(f"Tunnel detected: {has_tunnel}")
print(f"Max probability: {prob_map.max():.4f}")
```

## File Structure

```
GraviQ-quantum-sensing/
├── density_grid_generator.py  # Generate training data
├── dataset.py                 # PyTorch dataset and dataloader
├── model.py                   # U-Net architecture
├── train.py                   # Training script
├── inference.py               # Evaluation and prediction
├── requirements.txt           # Python dependencies
├── README_training.md         # This file
│
├── training_data/             # Generated by density_grid_generator.py
│   ├── density_grid_000.npy
│   ├── tunnel_mask_000.npy
│   ├── metadata_000.json
│   └── sample_000.png
│
├── checkpoints/               # Model checkpoints
│   ├── best_model.pth
│   └── checkpoint_epoch_X.pth
│
├── runs/                      # TensorBoard logs
└── evaluation_results/        # Evaluation outputs
    ├── metrics.json
    └── sample_XXX.png
```

## Model Architecture

**U-Net** for semantic segmentation:
- Input: (B, 1, 60, 150) - density grid
- Output: (B, 1, 60, 150) - tunnel probability map
- ~380K trainable parameters

**Loss Function:**
- Combined BCE + Dice Loss (50/50 weighted)

**Metrics:**
- **Dice Score**: Overlap between prediction and ground truth
- **IoU**: Intersection over Union
- **Classification Accuracy**: Correct tunnel presence detection

## Training Tips

### 1. More Training Data
Generate more samples for better performance:
```python
# In density_grid_generator.py, change:
num_samples = 500  # or more
```

### 2. Adjust Class Balance
If you want more samples with tunnels:
```python
# In density_grid_generator.py, make_grid():
# Weighted random choice
if rng.random() < 0.7:  # 70% chance of tunnel
    num_tunnels = rng.randint(1, 4)
else:
    num_tunnels = 0
```

### 3. Data Augmentation
Add to `dataset.py`:
- Random horizontal flips
- Random noise
- Random scaling

### 4. Hyperparameter Tuning
- Learning rate: try 1e-4 or 5e-4
- Batch size: try 16 or 32 (if memory allows)
- Loss weights: adjust BCE/Dice ratio

### 5. Early Stopping
Monitor validation loss and stop if no improvement for 20 epochs.

## Expected Results

With 50 training samples:
- **Classification Accuracy**: 85-95%
- **Dice Score**: 0.70-0.85
- **IoU**: 0.60-0.75

With 500+ samples:
- **Classification Accuracy**: 95%+
- **Dice Score**: 0.85+
- **IoU**: 0.75+

## Next Steps

1. **Generate more data**: Increase `num_samples` to 500+
2. **Train model**: Run `python train.py`
3. **Monitor**: Watch TensorBoard for convergence
4. **Evaluate**: Run `python inference.py`
5. **Deploy**: Use trained model for real gravity sensing data

## Troubleshooting

**CUDA out of memory:**
- Reduce `batch_size` in `train.py`
- Use CPU by setting `device='cpu'`

**Model not learning:**
- Check data distribution (ensure both positive and negative examples)
- Reduce learning rate
- Check loss function weights

**Poor generalization:**
- Generate more training data
- Add data augmentation
- Reduce model complexity (fewer features in U-Net)
